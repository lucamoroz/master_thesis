 
@BOOK{DUMMY:1,
AUTHOR="John Doe",
TITLE="The Book without Title",
PUBLISHER="Dummy Publisher",
YEAR="2100",
}


@article{STL,
	author ={Cleveland R. B. and Cleveland W. S. and McRa J. E. and Terpenning I. J.},
	title = {STL: A seasonal-trend decomposition procedure based on Loess},
	journaltitle = {Journal of Official Statistics},
	date =1990,
}

@Article{ExponentialSmoothingHoltCharles,
  author={Holt, Charles C.},
  title={{Forecasting seasonals and trends by exponentially weighted moving averages}},
  journal={International Journal of Forecasting},
  year=2004,
  volume={20},
  number={1},
  pages={5-10},
  month={},
  keywords={},
  doi={},
  abstract={No abstract is available for this item.},
  url={https://ideas.repec.org/a/eee/intfor/v20y2004i1p5-10.html}
}

@article{BoxJenkins,
author = {MAKRIDAKIS, SPYROS and HIBON, MICHÈLE},
title = {ARMA Models and the Box–Jenkins Methodology},
journal = {Journal of Forecasting},
volume = {16},
number = {3},
pages = {147-163},
keywords = {time-series forecasting, ARMA models, Box–Jenkins, empirical studies, M-Competition},
doi = {https://doi.org/10.1002/(SICI)1099-131X(199705)16:3<147::AID-FOR652>3.0.CO;2-X},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291099-131X%28199705%2916%3A3%3C147%3A%3AAID-FOR652%3E3.0.CO%3B2-X},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291099-131X%28199705%2916%3A3%3C147%3A%3AAID-FOR652%3E3.0.CO%3B2-X},
abstract = {Abstract The purpose of this paper is to apply the Box–Jenkins methodology to ARIMA models and determine the reasons why in empirical tests it is found that the post-sample forecasting the accuracy of such models is generally worse than much simpler time series methods. The paper concludes that the major problem is the way of making the series stationary in its mean (i.e. the method of differencing) that has been proposed by Box and Jenkins. If alternative approaches are utilized to remove and extrapolate the trend in the data, ARMA models outperform the models selected through Box–Jenkins methodology. In addition, it is shown that using ARMA models to seasonally adjusted data slightly improves post-sample accuracies while simplifying the use of ARMA models. It is also confirmed that transformations slightly improve post-sample forecasting accuracy, particularly for long forecasting horizons. Finally, it is demonstrated that AR(1), AR(2) and ARMA(1,1) models can produce more accurate post-sample forecasts than those found through the application of Box–Jenkins methodology.© 1997 John Wiley \& Sons, Ltd.},
year = {1997}
}

@INPROCEEDINGS{ForecastingSurvey,  author={Mahalakshmi, G. and Sridevi, S. and Rajaram, S.},  booktitle={2016 International Conference on Computing Technologies and Intelligent Data Engineering (ICCTIDE'16)},   title={A survey on forecasting of time series data},   year={2016},  volume={},  number={},  pages={1-8},  doi={10.1109/ICCTIDE.2016.7725358}}

@article{25YearsForecasting,
title = {25 years of time series forecasting},
journal = {International Journal of Forecasting},
volume = {22},
number = {3},
pages = {443-473},
year = {2006},
note = {Twenty five years of forecasting},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2006.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169207006000021},
author = {Jan G. {De Gooijer} and Rob J. Hyndman},
keywords = {Accuracy measures, ARCH, ARIMA, Combining, Count data, Densities, Exponential smoothing, Kalman filter, Long memory, Multivariate, Neural nets, Nonlinearity, Prediction intervals, Regime-switching, Robustness, Seasonality, State space, Structural models, Transfer function, Univariate, VAR},
abstract = {We review the past 25 years of research into time series forecasting. In this silver jubilee issue, we naturally highlight results published in journals managed by the International Institute of Forecasters (Journal of Forecasting 1982–1985 and International Journal of Forecasting 1985–2005). During this period, over one third of all papers published in these journals concerned time series forecasting. We also review highly influential works on time series forecasting that have been published elsewhere during this period. Enormous progress has been made in many areas, but we find that there are a large number of topics in need of further development. We conclude with comments on possible future research directions in this field.}
}

@article{AutoForecasting,
 title={Automatic Time Series Forecasting: The forecast Package for R},
 volume={27},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v027i03},
 doi={10.18637/jss.v027.i03},
 abstract={Automatic forecasts of large numbers of univariate time series are often needed in business and other contexts. We describe two automatic forecasting algorithms that have been implemented in the forecast package for R. The first is based on innovations state space models that underly exponential smoothing methods. The second is a step-wise algorithm for forecasting with ARIMA models. The algorithms are applicable to both seasonal and non-seasonal data, and are compared and illustrated using four real time series. We also briefly describe some of the other functionality available in the forecast package.},
 number={3},
 journal={Journal of Statistical Software},
 author={Hyndman, Rob J. and Khandakar, Yeasmin},
 year={2008},
 pages={1–22}
}

@article{GluonTS,
  author    = {Alexander Alexandrov and
               Konstantinos Benidis and
               Michael Bohlke{-}Schneider and
               Valentin Flunkert and
               Jan Gasthaus and
               Tim Januschowski and
               Danielle C. Maddix and
               Syama Sundar Rangapuram and
               David Salinas and
               Jasper Schulz and
               Lorenzo Stella and
               Ali Caner T{\"{u}}rkmen and
               Yuyang Wang},
  title     = {GluonTS: Probabilistic Time Series Models in Python},
  journal   = {CoRR},
  volume    = {abs/1906.05264},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.05264},
  eprinttype = {arXiv},
  eprint    = {1906.05264},
  timestamp = {Sun, 13 Jun 2021 10:37:51 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-05264.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DeepState,
 author = {Rangapuram, Syama Sundar and Seeger, Matthias W and Gasthaus, Jan and Stella, Lorenzo and Wang, Yuyang and Januschowski, Tim},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep State Space Models for Time Series Forecasting},
 url = {https://proceedings.neurips.cc/paper/2018/file/5cf68969fb67aa6082363a6d4e6468e2-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{DeepAR,
  author    = {Valentin Flunkert and
               David Salinas and
               Jan Gasthaus},
  title     = {DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks},
  journal   = {CoRR},
  volume    = {abs/1704.04110},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.04110},
  eprinttype = {arXiv},
  eprint    = {1704.04110},
  timestamp = {Mon, 13 Aug 2018 16:46:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/FlunkertSG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{MQCNN,
	title={A Multi-Horizon Quantile Recurrent Forecaster}, 
	author={Ruofeng Wen and Kari Torkkola and Balakrishnan Narayanaswamy and Dhruv Madeka},
	year={2018},
	eprint={1711.11053},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@article{FacebookProphet,
author = {Taylor, Sean and Letham, Benjamin},
year = {2017},
month = {09},
pages = {},
title = {Forecasting at Scale},
volume = {72},
journal = {The American Statistician},
doi = {10.1080/00031305.2017.1380080}
}

@article{GAM,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2289439},
 abstract = {Generalized additive models have the form η( x) = α + ∑ fj(xj), where η might be the regression function in a multiple regression or the logistic transformation of the posterior probability $\Pr(y = 1 \mid \mathbf x)$ in a logistic regression. In fact, these models generalize the whole family of generalized linear models η( x) = β' x, where η( x) = g(μ( x)) is some transformation of the regression function. We use the local scoring algorithm to estimate the functions fj(xj) nonparametrically, using a scatterplot smoother as a building block. We demonstrate the models in two different analyses: a nonparametric analysis of covariance and a logistic regression. The procedure can be used as a diagnostic tool for identifying parametric transformations of the covariates in a standard linear analysis. A variety of inferential tools have been developed to aid the analyst in assessing the relevance and significance of the estimated functions: these include confidence curves, degrees of freedom estimates, and approximate hypothesis tests. The local scoring algorithm is analogous to the iterative reweighted least squares algorithm for solving likelihood and nonlinear regression equations. At each iteration, an adjusted dependent variable is formed and an additive regression model is fit using the backfitting algorithm. The backfitting algorithm cycles through the variables and estimates each coordinated function by smoothing the partial residuals.},
 author = {Trevor Hastie and Robert Tibshirani},
 journal = {Journal of the American Statistical Association},
 number = {398},
 pages = {371--386},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Generalized Additive Models: Some Applications},
 volume = {82},
 year = {1987}
}

@misc{MicrosoftSSA,
  title = {SSAForecaster},
  howpublished = {https://docs.microsoft.com/en-us/python/api/nimbusml/nimbusml.timeseries.ssaforecaster?view=nimbusml-py-latest}
}

@inbook{ExponentialSmoothingStateSpace,
author = {Hyndman, Rob and Koehler, Anne and Ord, Keith and Snyder, Ralph},
year = {2008},
month = {01},
pages = {},
title = {Forecasting with exponential smoothing. The state space approach},
doi = {10.1007/978-3-540-71918-2}
}

@article{DeepLearningForecastingSurvey,
author = {Lim, Bryan  and Zohren, Stefan },
title = {Time-series forecasting with deep learning: a survey},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
volume = {379},
number = {2194},
pages = {20200209},
year = {2021},
doi = {10.1098/rsta.2020.0209},

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2020.0209},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2020.0209}
,
    abstract = { Numerous deep learning architectures have been developed to accommodate the diversity of time-series datasets across different domains. In this article, we survey common encoder and decoder designs used in both one-step-ahead and multi-horizon time-series forecasting—describing how temporal information is incorporated into predictions by each model. Next, we highlight recent developments in hybrid deep learning models, which combine well-studied statistical models with neural network components to improve pure methods in either category. Lastly, we outline some ways in which deep learning can also facilitate decision support with time-series data. This article is part of the theme issue ‘Machine learning for weather and climate modelling’. }
}

@article{RNNLSTM,
  author    = {Alex Sherstinsky},
  title     = {Fundamentals of Recurrent Neural Network {(RNN)} and Long Short-Term
               Memory {(LSTM)} Network},
  journal   = {CoRR},
  volume    = {abs/1808.03314},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.03314},
  eprinttype = {arXiv},
  eprint    = {1808.03314},
  timestamp = {Sun, 02 Sep 2018 15:01:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-03314.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{NARX,
	title={Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies}, 
	author={Robert DiPietro and Christian Rupprecht and Nassir Navab and Gregory D. Hager},
	year={2018},
	eprint={1702.07805},
	archivePrefix={arXiv},
	primaryClass={cs.NE}
}

@article{EncoderDecoder,
  author    = {Kyunghyun Cho and
               Bart van Merrienboer and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               Fethi Bougares and
               Holger Schwenk and
               Yoshua Bengio},
  title     = {Learning Phrase Representations using {RNN} Encoder-Decoder for Statistical
               Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1406.1078},
  year      = {2014},
  url       = {http://arxiv.org/abs/1406.1078},
  eprinttype = {arXiv},
  eprint    = {1406.1078},
  timestamp = {Mon, 13 Aug 2018 16:46:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChoMGBSB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{MAKRIDAKIS2018802,
title = {The M4 Competition: Results, findings, conclusion and way forward},
journal = {International Journal of Forecasting},
volume = {34},
number = {4},
pages = {802-808},
year = {2018},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2018.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169207018300785},
author = {Spyros Makridakis and Evangelos Spiliotis and Vassilios Assimakopoulos},
keywords = {Forecasting competitions, M Competitions, Forecasting accuracy, Prediction intervals (PIs), Time series methods, Machine Learning (ML) methods, Benchmarking methods, Practice of forecasting},
abstract = {The M4 competition is the continuation of three previous competitions started more than 45 years ago whose purpose was to learn how to improve forecasting accuracy, and how such learning can be applied to advance the theory and practice of forecasting. The purpose of M4 was to replicate the results of the previous ones and extend them into three directions: First significantly increase the number of series, second include Machine Learning (ML) forecasting methods, and third evaluate both point forecasts and prediction intervals. The five major findings of the M4 Competitions are: 1. Out Of the 17 most accurate methods, 12 were “combinations” of mostly statistical approaches. 2. The biggest surprise was a “hybrid” approach that utilized both statistical and ML features. This method’s average sMAPE was close to 10% more accurate than the combination benchmark used to compare the submitted methods. 3. The second most accurate method was a combination of seven statistical methods and one ML one, with the weights for the averaging being calculated by a ML algorithm that was trained to minimize the forecasting. 4. The two most accurate methods also achieved an amazing success in specifying the 95% prediction intervals correctly. 5. The six pure ML methods performed poorly, with none of them being more accurate than the combination benchmark and only one being more accurate than Naïve2. This paper presents some initial results of M4, its major findings and a logical conclusion. Finally, it outlines what the authors consider to be the way forward for the field of forecasting.}
}

@article{UberHybridES,
title = {A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting},
journal = {International Journal of Forecasting},
volume = {36},
number = {1},
pages = {75-85},
year = {2020},
note = {M4 Competition},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2019.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S0169207019301153},
author = {Slawek Smyl},
keywords = {Forecasting competitions, M4, Dynamic computational graphs, Automatic differentiation, Long short term memory (LSTM) networks, Exponential smoothing},
abstract = {This paper presents the winning submission of the M4 forecasting competition. The submission utilizes a dynamic computational graph neural network system that enables a standard exponential smoothing model to be mixed with advanced long short term memory networks into a common framework. The result is a hybrid and hierarchical forecasting method.}
}

@article{M5Competition,
author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilis},
year = {2020},
month = {10},
pages = {},
title = {The M5 Accuracy competition: Results, findings and conclusions}
}

@book{ForecastingHyndmanAthanasopoulos,
title = "Forecasting: Principles and Practice",
author = "Hyndman, {Robin John} and George Athanasopoulos",
year = "2018",
language = "English",
publisher = "OTexts",
address = "Australia",
edition = "2nd",
}

@book{ForecastingBoxJenkins,
	title = "Time Series Analysis: Forecasting and Control",
	author = "George E. P. Box and Gwilym M. Jenkins and Gregory C. Reinsel and Greta M. Ljung",
	year = "2015",
	language = "English",
	edition = "5th",
}

@article{DiscriminativeGenerativeModels,
	title = {On discriminative vs. generative classifiers: a comparison of logistic regression and naive Bayes},
	journal = {Advances in Neural Information Processing Systems},
	number = {14},
	pages = {841-848},
	year = {2002},
	author = {Andrew Y. Ng and Micheal I. Jordan},
}

@misc{seq2seq,
	title={Sequence to Sequence Learning with Neural Networks}, 
	author={Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
	year={2014},
	eprint={1409.3215},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{RNN,
	title = {Finding structure in time},
	journal = {Cognitive Science},
	volume = {14},
	number = {2},
	pages = {179-211},
	year = {1990},
	issn = {0364-0213},
	doi = {https://doi.org/10.1016/0364-0213(90)90002-E},
	url = {https://www.sciencedirect.com/science/article/pii/036402139090002E},
	author = {Jeffrey L. Elman},
	abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.}
}

@article{RNNForecasting,
	author    = {Hansika Hewamalage and
	Christoph Bergmeir and
	Kasun Bandara},
	title     = {Recurrent Neural Networks for Time Series Forecasting: Current Status
	and Future Directions},
	journal   = {CoRR},
	volume    = {abs/1909.00590},
	year      = {2019},
	url       = {http://arxiv.org/abs/1909.00590},
	eprinttype = {arXiv},
	eprint    = {1909.00590},
	timestamp = {Mon, 16 Sep 2019 17:27:14 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1909-00590.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Wavenet,
	author    = {A{\"{a}}ron van den Oord and
	Sander Dieleman and
	Heiga Zen and
	Karen Simonyan and
	Oriol Vinyals and
	Alex Graves and
	Nal Kalchbrenner and
	Andrew W. Senior and
	Koray Kavukcuoglu},
	title     = {WaveNet: {A} Generative Model for Raw Audio},
	journal   = {CoRR},
	volume    = {abs/1609.03499},
	year      = {2016},
	url       = {http://arxiv.org/abs/1609.03499},
	eprinttype = {arXiv},
	eprint    = {1609.03499},
	timestamp = {Thu, 14 Oct 2021 09:15:04 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/OordDZSVGKSK16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{L-BFGS,
	author = {Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
	title = {A Limited Memory Algorithm for Bound Constrained Optimization},
	journal = {SIAM Journal on Scientific Computing},
	volume = {16},
	number = {5},
	pages = {1190-1208},
	year = {1995},
	doi = {10.1137/0916069},
	URL = { 
	https://doi.org/10.1137/0916069
	},
	eprint = { 
	https://doi.org/10.1137/0916069
	}	
}

@article{VanishingGradient,
	author = {Hochreiter, Sepp},
	title = {The Vanishing Gradient Problem during Learning Recurrent Neural Nets and Problem Solutions},
	year = {1998},
	issue_date = {April 1998},
	publisher = {World Scientific Publishing Co., Inc.},
	address = {USA},
	volume = {6},
	number = {2},
	issn = {0218-4885},
	url = {https://doi.org/10.1142/S0218488598000094},
	doi = {10.1142/S0218488598000094},
	journal = {Int. J. Uncertain. Fuzziness Knowl.-Based Syst.},
	month = {apr},
	pages = {107–116},
	numpages = {10},
	keywords = {recurrent neural nets, vanishing gradient, long short-term memory, long-term dependencies}
}

@article{CloudComputing,
	author = {Armbrust, Michael and Fox, Armando and Griffith, Rean and Joseph, Anthony D. and Katz, Randy and Konwinski, Andy and Lee, Gunho and Patterson, David and Rabkin, Ariel and Stoica, Ion and Zaharia, Matei},
	title = {A View of Cloud Computing},
	year = {2010},
	issue_date = {April 2010},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {53},
	number = {4},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/1721654.1721672},
	doi = {10.1145/1721654.1721672},
	abstract = {Clearing the clouds away from the true potential and obstacles posed by this computing capability.},
	journal = {Commun. ACM},
	month = {apr},
	pages = {50–58},
	numpages = {9}
}

@ARTICLE{ArimaWorkloadPrediction,  author={Calheiros, Rodrigo N. and Masoumi, Enayat and Ranjan, Rajiv and Buyya, Rajkumar},  journal={IEEE Transactions on Cloud Computing},   title={Workload Prediction Using ARIMA Model and Its Impact on Cloud Applications’ QoS},   year={2015},  volume={3},  number={4},  pages={449-458},  doi={10.1109/TCC.2014.2350475}}

@INPROCEEDINGS{ArmaAutoscaling,  author={Roy, Nilabja and Dubey, Abhishek and Gokhale, Aniruddha},  booktitle={2011 IEEE 4th International Conference on Cloud Computing},   title={Efficient Autoscaling in the Cloud Using Predictive Models for Workload Forecasting},   year={2011},  volume={},  number={},  pages={500-507},  doi={10.1109/CLOUD.2011.42}}

@INPROCEEDINGS{WorkloadCharacterizationAndPrediction,  author={Khan, Arijit and Yan, Xifeng and Tao, Shu and Anerousis, Nikos},  booktitle={2012 IEEE Network Operations and Management Symposium},   title={Workload characterization and prediction in the cloud: A multiple time series approach},   year={2012},  volume={},  number={},  pages={1287-1294},  doi={10.1109/NOMS.2012.6212065}}

@ARTICLE{LSTMLargeScaleWorkloadForecasting,  author={Tang, Xiaoyong},  journal={IEEE Access},   title={Large-Scale Computing Systems Workload Prediction Using Parallel Improved LSTM Neural Network},   year={2019},  volume={7},  number={},  pages={40525-40533},  doi={10.1109/ACCESS.2019.2905634}}

@article{Seagull,
	author    = {Olga Poppe and
	Tayo Amuneke and
	Dalitso Banda and
	Aritra De and
	Ari Green and
	Manon Knoertzer and
	Ehi Nosakhare and
	Karthik Rajendran and
	Deepak Shankargouda and
	Meina Wang and
	Alan Au and
	Carlo Curino and
	Qun Guo and
	Alekh Jindal and
	Ajay Kalhan and
	Morgan Oslake and
	Sonia Parchani and
	Vijay Ramani and
	Raj Sellappan and
	Saikat Sen and
	Sheetal Shrotri and
	Soundararajan Srinivasan and
	Ping Xia and
	Shize Xu and
	Alicia Yang and
	Yiwen Zhu},
	title     = {Seagull: An Infrastructure for Load Prediction and Optimized Resource
	Allocation},
	journal   = {CoRR},
	volume    = {abs/2009.12922},
	year      = {2020},
	url       = {https://arxiv.org/abs/2009.12922},
	eprinttype = {arXiv},
	eprint    = {2009.12922},
	timestamp = {Wed, 30 Sep 2020 16:16:22 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2009-12922.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{BO,  author={Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},  journal={Proceedings of the IEEE},   title={Taking the Human Out of the Loop: A Review of Bayesian Optimization},   year={2016},  volume={104},  number={1},  pages={148-175},  doi={10.1109/JPROC.2015.2494218}}

@inproceedings{CGPBanditOptimization,
	author = {Krause, Andreas and Ong, Cheng},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Contextual Gaussian Process Bandit Optimization},
	url = {https://proceedings.neurips.cc/paper/2011/file/f3f1b7fc5a8779a9e618e1f23a7b7860-Paper.pdf},
	volume = {24},
	year = {2011}
}

@article{AkamasCGP,
	author = {Cereda, Stefano and Valladares, Stefano and Cremonesi, Paolo and Doni, Stefano},
	title = {CGPTuner: A Contextual Gaussian Process Bandit Approach for the Automatic Tuning of IT Configurations under Varying Workload Conditions},
	year = {2021},
	issue_date = {April 2021},
	publisher = {VLDB Endowment},
	volume = {14},
	number = {8},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3457390.3457404},
	doi = {10.14778/3457390.3457404},
	abstract = {Properly selecting the configuration of a database management system (DBMS) is essential to increase performance and reduce costs. However, the task is astonishingly tricky due to a large number of tunable configuration parameters and their inter-dependencies. Also, the optimal configuration depends upon the workload to which the DBMS is exposed. To extract the full potential of a DBMS, we must also consider the entire IT stack on which the DBMS is running, comprising layers like the Java virtual machine, the operating system and the physical machine. Each layer offers a multitude of parameters that we should take into account. The available parameters vary as new software versions are released, making it impractical to rely on historical knowledge bases. We present a novel tuning approach for the DBMS configuration auto-tuning that quickly finds a well-performing configuration of an IT stack and adapts it to workload variations, without having to rely on a knowledge base. We evaluate the proposed approach using the Cassandra and MongoDB DBMSs, showing that it adjusts the suggested configuration to the observed workload and is portable across different IT applications. We try to minimise the memory consumption without increasing the response time, showing that the proposed approach reduces the response time and increases the memory requirements only under heavy-load conditions, reducing it again when the load decreases.},
	journal = {Proc. VLDB Endow.},
	month = {apr},
	pages = {1401–1413},
	numpages = {13}
}

@article{WorkloadCharacterization,
	author = {Calzarossa, Maria Carla and Massari, Luisa and Tessera, Daniele},
	title = {Workload Characterization: A Survey Revisited},
	year = {2016},
	issue_date = {February 2016},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {48},
	number = {3},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/2856127},
	doi = {10.1145/2856127},
	abstract = {Workload characterization is a well-established discipline that plays a key role in many performance engineering studies. The large-scale social behavior inherent in the applications and services being deployed nowadays leads to rapid changes in workload intensity and characteristics and opens new challenging management and performance issues. A deep understanding of user behavior and workload properties and patterns is therefore compelling. This article presents a comprehensive survey of the state of the art of workload characterization by addressing its exploitation in some popular application domains. In particular, we focus on conventional web workloads as well as on the workloads associated with online social networks, video services, mobile apps, and cloud computing infrastructures. We discuss the peculiarities of these workloads and present the methodological approaches and modeling techniques applied for their characterization. The role of workload models in various scenarios (e.g., performance evaluation, capacity planning, content distribution, resource provisioning) is also analyzed.},
	journal = {ACM Comput. Surv.},
	month = {feb},
	articleno = {48},
	numpages = {43},
	keywords = {graph analysis, user behavior, online social networks, web workload, workload measurements, cloud computing, statistical techniques, mobile apps, video services, performance evaluation, Workload characterization}
}

@ARTICLE{ClusteringSurvey,  author={Rui Xu and Wunsch, D.},  journal={IEEE Transactions on Neural Networks},   title={Survey of clustering algorithms},   year={2005},  volume={16},  number={3},  pages={645-678},  doi={10.1109/TNN.2005.845141}}

@article{PCA,
	title = {Principal components analysis (PCA)},
	journal = {Computers \& Geosciences},
	volume = {19},
	number = {3},
	pages = {303-342},
	year = {1993},
	issn = {0098-3004},
	doi = {https://doi.org/10.1016/0098-3004(93)90090-R},
	url = {https://www.sciencedirect.com/science/article/pii/009830049390090R},
	author = {Andrzej Maćkiewicz and Waldemar Ratajczak},
	keywords = {Principal Components Analysis, Variance-covariance matrix, Coefficients of determination, Eigenvalues, Eigenvectors, Correlation matrix, Bartlett's statistics, FORTRAN 77},
	abstract = {Principal Components Analysis (PCA) as a method of multivariate statistics was created before the Second World War. However, the wider application of this method only occurred in the 1960s, during the “Quantitative Revolution” in the Natural and Social Sciences. The main reason for this time-lag was the huge difficulty posed by calculations involving this method. Only with the advent and development of computers did the almost unlimited application of multivariate statistical methods, including principal components, become possible. At the same time, requirements arose for precise numerical methods concerning, among other things, the calculation of eigenvalues and eigenvectors, because the application of principal components to technical problems required absolute accuracy. On the other hand, numerous applications in Social Sciences gave rise to a significant increase in the ability to interpret these nonobservable variables, which is just what the principal components are. In the application of principal components, the problem is not only to do with their formal properties but above all, their empirical origins. The authors considered these two tendencies during the creation of the program for principal components. This program—entitled PCA—accompanies this paper. It analyzes consecutively, matrices of variance-covariance and correlations, and performs the following functions: •- the determination of eigenvalues and eigenvectors of these matrices.•- the testing of principal components.•- the calculation of coefficients of determination between selected components and the initial variables, and the testing of these coefficients,•- the determination of the share of variation of all the initial variables in the variation of particular components,•- construction of a dendrite for the initial set of variables,•- the construction of a dendrite for a selected pattern of the principal components,•- the scatter of the objects studied in a selected coordinate system. Thus, the PCA program performs many more functions especially in testing and graphics, than PCA programs in conventional statistical packages. Included in this paper are a theoretical description of principal components, the basic rules for their interpretation and also statistical testing.}
}

@ARTICLE{WorkloadsPowerLaw,  author={Mahanti, Aniket and Carlsson, Niklas and Mahanti, Anirban and Arlitt, Martin and Williamson, Carey},  journal={IEEE Network},   title={A tale of the tails: Power-laws in internet measurements},   year={2013},  volume={27},  number={1},  pages={59-64},  doi={10.1109/MNET.2013.6423193}}

@Inbook{PearsonCoefficient,
	editor="Kirch, Wilhelm",
	title="Pearson's Correlation Coefficient",
	bookTitle="Encyclopedia of Public Health",
	year="2008",
	publisher="Springer Netherlands",
	address="Dordrecht",
	pages="1090--1091",
	isbn="978-1-4020-5614-7",
	doi="10.1007/978-1-4020-5614-7_2569",
	url="https://doi.org/10.1007/978-1-4020-5614-7_2569"
}

